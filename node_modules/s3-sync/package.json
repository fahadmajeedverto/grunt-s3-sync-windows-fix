{
  "name": "s3-sync",
  "version": "0.3.0",
  "description": "A streaming upload tool for Amazon S3",
  "main": "index.js",
  "dependencies": {
    "queue-async": "~1.0.4",
    "crypto": "0.0.3",
    "mime": "~1.2.9",
    "backoff": "~2.3.0",
    "xtend": "~2.0.6",
    "knox": "~0.8.3",
    "event-stream": "3.0.16",
    "once": "~1.1.1"
  },
  "peerDependencies": {
    "level": "0.x.x"
  },
  "devDependencies": {},
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "repository": {
    "type": "git",
    "url": "git://github.com/hughsk/s3-sync.git"
  },
  "keywords": [
    "stream",
    "s3",
    "upload",
    "sync",
    "directory"
  ],
  "author": {
    "name": "Hugh Kennedy",
    "email": "hughskennedy@gmail.com",
    "url": "http://hughskennedy.com/"
  },
  "license": "MIT",
  "readmeFilename": "README.md",
  "readme": "# s3-sync #\n\nA streaming upload tool for Amazon S3, taking input from a\n[`readdirp`](http://npmjs.org/package/readdirp) stream, and outputting the\nresulting files.\n\ns3-sync is also optionally backed by a [level](http://github.com/level/level)\ndatabase to use as a local cache for file uploads. This way, you can minimize\nthe frequency you have to hit S3 and speed up the whole process considerably.\n\nYou can use this to sync complete directory trees with S3 when deploying static\nwebsites. It's a work in progress, so expect occasional API changes and\nadditional features.\n\n## Installation ##\n\n``` bash\nnpm install s3-sync\n```\n\n## Usage ##\n\n### `require('s3-sync').createStream([db, ]options)` ###\n\nCreates an upload stream. Passes its options to [knox](http://ghub.io/knox),\nso at a minimum you'll need:\n\n* `key`: Your AWS access key.\n* `secret`: Your AWS secret.\n* `bucket`: The bucket to upload to.\n\nThe following are also specific to s3-sync:\n\n* `concurrency`: The maximum amount of files to upload concurrently.\n* `headers`: Additional headers to include on each file.\n* `hashKey`: By default, file hashes are stored based on the file's absolute\n  path. This doesn't work very nicely with temporary files, so you can pass\n  this function in to map the file object to a string key for the hash.\n\nYou can also store your local cache in S3, provided you pass the following\noptions, and use `getCache` and `putCache` (see below) before/after uploading:\n\n* `cacheDest`: the path to upload your cache backup to in S3.\n* `cacheSrc`: the local, temporary, text file to stream to before uploading to\n  S3.\n\nIf you want more control over the files and their locations that you're\nuploading, you can write file objects directly to the stream, e.g.:\n\n``` javascript\nvar stream = s3sync({\n    key: process.env.AWS_ACCESS_KEY\n  , secret: process.env.AWS_SECRET_KEY\n  , bucket: 'sync-testing'\n})\n\nstream.write({\n    src: __filename\n  , dest: '/uploader.js'\n})\n\nstream.end({\n    src: __dirname + '/README.md'\n  , dest: '/README.md'\n})\n```\n\nWhere `src` is the *absolute* local file path, and `dest` is the location to\nupload the file to on the S3 bucket.\n\n`db` is an optional argument - pass it a *level* database and it'll keep a\nlocal cache of file hashes, keeping S3 requests to a minimum.\n\n### `stream.putCache(callback)` ###\n\nUploads your level cache, if available, to the S3 bucket. This means that your\ncache only needs to be populated once.\n\n### `stream.getCache(callback)` ###\n\nStreams a previously uploaded cache from S3 to your local level database.\n\n### `stream.on('fail', callback)` ###\n\nEmitted when a file has failed to upload. This will be called each time the\nfile is attempted to be uploaded.\n\n## Example ##\n\nHere's an example using `level` and `readdirp` to upload a local directory to\nan S3 bucket:\n\n``` javascript\nvar level = require('level')\n  , s3sync = require('s3-sync')\n  , readdirp = require('readdirp')\n\n// To cache the S3 HEAD results and speed up the\n// upload process. Usage is optional.\nvar db = level(__dirname + '/cache')\n\nvar files = readdirp({\n    root: __dirname\n  , directoryFilter: ['!.git', '!cache']\n})\n\n// Takes the same options arguments as `knox`,\n// plus some additional options listed above\nvar uploader = s3sync(db, {\n    key: process.env.AWS_ACCESS_KEY\n  , secret: process.env.AWS_SECRET_KEY\n  , bucket: 'sync-testing'\n  , concurrency: 16\n}).on('data', function(file) {\n  console.log(file.fullPath + ' -> ' + file.url)\n})\n\nfiles.pipe(uploader)\n```\n\nYou can find another example which includes remote cache storage at\n[example.js](https://github.com/hughsk/s3-sync/blob/master/example.js).\n",
  "bugs": {
    "url": "https://github.com/hughsk/s3-sync/issues"
  },
  "_id": "s3-sync@0.3.0",
  "_from": "s3-sync@~0.3.0"
}
